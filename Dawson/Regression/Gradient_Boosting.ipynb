{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from pyearth import Earth\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data set and creat feature and target space\n",
    "df = pd.read_csv(r'Volumetric_features.csv')\n",
    "X=df.drop(columns=['S.No','dataset','Age'])\n",
    "y=df['Age']\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new =df.drop(columns=['S.No','dataset'])\n",
    "correlation_matrix = df_new.corr()\n",
    "correlation_matrix['Age']\n",
    "\n",
    "cor_target = abs(correlation_matrix['Age']) \n",
    "\n",
    "features_to_drop = cor_target[cor_target<0.2] #script to eliminate features with low correlation\n",
    "to_drop_frame = features_to_drop.to_frame()\n",
    "row_names = to_drop_frame.index\n",
    "row_names_list = list(row_names)\n",
    "row_names_list.append('Age')\n",
    "row_names_list.append('S.No')\n",
    "row_names_list.append('dataset')\n",
    "y = df['Age'].values\n",
    "x = df.drop(row_names_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data space for model with and without feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\n",
    "X_train2,X_test2,y_train2,y_test2 = train_test_split(x,y,test_size=0.25) #model with feature selection\n",
    "\n",
    "#scaling features\n",
    "scaler = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler2.fit(X_train2)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train2 = scaler2.transform(X_train2)\n",
    "X_test2 = scaler2.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize Model Parameters\n",
    "learn_rate = [0.001,0.01,0.1]\n",
    "n_est = [10,50,100,300,500,800,1000]\n",
    "\n",
    "for i in learn_rate:\n",
    "    print()\n",
    "    for j in n_est:\n",
    "        grad_model = GradientBoostingRegressor(learning_rate=i,n_estimators=j)\n",
    "        grad_model.fit(X_train,y_train)\n",
    "        print(f'For learning rate {i} and n_estimators {j}, the model score is {grad_model.score(X_test,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_score1=[0.012,0.065,0.125,0.317,0.453,0.588,0.648]\n",
    "m_score2=[0.126,0.454,0.649,0.817,0.838,0.850,0.854]\n",
    "m_score3=[0.663,0.838,0.855,0.865,0.869,0.870,0.871]\n",
    "plt.plot(n_est,m_score1)\n",
    "plt.plot(n_est,m_score2)\n",
    "plt.plot(n_est,m_score3)\n",
    "plt.legend(['LR=0.001','LR=0.01','LR=0.1'])\n",
    "plt.xlabel('Number of estimators used')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('Parameter Optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build models based on optimized parameters\n",
    "grad_model = GradientBoostingRegressor(learning_rate=0.1,n_estimators=800)\n",
    "grad_model.fit(X_train,y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test,grad_model.predict(X_test))\n",
    "r2 = grad_model.score(X_test,y_test)\n",
    "\n",
    "grad_model2 = GradientBoostingRegressor(learning_rate=0.1,n_estimators=800)\n",
    "grad_model2.fit(X_train2,y_train2)\n",
    "\n",
    "mse2 = mean_squared_error(y_test2,grad_model2.predict(X_test2))\n",
    "r2_2 = grad_model2.score(X_test2,y_test2)\n",
    "\n",
    "print(f'Model accuracies w/o feature detection: R^2 value is {r2:.3f} and the MSE is {mse:.3f}') #using all predictors\n",
    "print(f'Model accuracies w/ feature detection: R^2 value is {r2_2:.3f} and the MSE is {mse2:.3f}') #using cov<0.2 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance for the model\n",
    "X_train2 = pd.DataFrame(X_train2, columns=x.columns)\n",
    "\n",
    "importance = grad_model2.feature_importances_ \n",
    "indices=np.argsort(importance)[::-1]\n",
    "#for i,v in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f'% (i,v))\n",
    "plt.bar([x for x in range(len(importance))],importance)\n",
    "#plt.xticks(range(X_train2.shape[1]),X_train2.columns[indices],rotation=90,fontsize=6)\n",
    "plt.ylabel('Coefficient Weight')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "print('The feature with the highest weighting in the model is WM-hypointensities')\n",
    "print('Note this was found by printing feature weights and identifying feature name by printing column name in list')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
